{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrador/anaconda3/envs/deeplung/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# univariate mlp example\n",
    "from numpy import array\n",
    "\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dropout, LSTM\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.externals import joblib \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/administrador/anaconda3/envs/deeplung/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator StandardScaler from version 0.20.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = joblib.load(PATH/'train_scaler_x.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8126, 2500) (8126, 1250)\n",
      "(1016, 2500) (1016, 1250)\n",
      "(1016, 2500) (1016, 1250)\n",
      "[1.0743 1.0802 1.1212 1.2092 1.35   1.5396 1.7664 2.0088 2.2473 2.4399]\n"
     ]
    }
   ],
   "source": [
    "dfX_Train = np.load(PATH/'train_x.npy')\n",
    "dfX_Val = np.load(PATH/'val_x.npy') \n",
    "dfX_Test = np.load(PATH/'test_x.npy')\n",
    "dfY_Train = np.load(PATH/'train_y.npy')\n",
    "dfY_Val = np.load(PATH/'val_y.npy')\n",
    "dfY_Test = np.load(PATH/'test_y.npy')\n",
    "print(dfX_Train.shape, dfY_Train.shape)\n",
    "print(dfX_Val.shape, dfY_Val.shape)\n",
    "print(dfX_Test.shape, dfY_Test.shape)\n",
    "print(dfX_Train[0,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=None\n",
    "freq = 125\n",
    "ini = 0 #freq\n",
    "dim = freq * 2\n",
    "ecg = 1250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize Y\n",
    "rng = (dfY_Train[:,ini:ini+dim].max(axis=1) - dfY_Train[:,ini:ini+dim].min(axis=1))\n",
    "trainY_n = np.array([dfY_Train [:,i] - dfY_Train[:,ini:ini+dim].min(axis=1) \n",
    "                       for i in range(ini,ini+dim)]) / rng\n",
    "\n",
    "rng = (dfY_Val[:,ini:ini+dim].max(axis=1) - dfY_Val[:,ini:ini+dim].min(axis=1))\n",
    "valY_n = np.array([dfY_Val [:,i] - dfY_Val[:,ini:ini+dim].min(axis=1) \n",
    "                       for i in range(ini,ini+dim)]) / rng\n",
    "\n",
    "rng = (dfY_Test[:,ini:ini+dim].max(axis=1) - dfY_Test[:,ini:ini+dim].min(axis=1))\n",
    "testY_n = np.array([dfY_Test [:,i] - dfY_Test[:,ini:ini+dim].min(axis=1) \n",
    "                       for i in range(ini,ini+dim)]) / rng\n",
    "\n",
    "trainY_n = trainY_n.T\n",
    "valY_n = valY_n.T\n",
    "testY_n = testY_n.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8126, 250) (8126, 250)\n",
      "(1016, 250) (1016, 250)\n",
      "(1016, 250) (1016, 250)\n"
     ]
    }
   ],
   "source": [
    "#Normalize X\n",
    "rng = (dfX_Train[:,ini:ini+dim].max(axis=1) - dfX_Train[:,ini:ini+dim].min(axis=1))\n",
    "trainX_0_n = np.array([dfX_Train [:,i] - dfX_Train[:,ini:ini+dim].min(axis=1) \n",
    "                       for i in range(ini,ini+dim)]) / rng\n",
    "\n",
    "rng = (dfX_Train[:,ecg+ini:ini+ecg+dim].max(axis=1) - dfX_Train[:,ecg+ini:ini+ecg+dim].min(axis=1))\n",
    "trainX_1_n = np.array([dfX_Train [:,i] - dfX_Train[:,ini+ecg:ini+ecg+dim].min(axis=1) for i in range(ini+ecg,ini+ecg+dim)]) / rng\n",
    "\n",
    "rng = (dfX_Val[:,ini:ini+dim].max(axis=1) - dfX_Val[:,ini:ini+dim].min(axis=1))\n",
    "valX_0_n = np.array([dfX_Val [:,i] - dfX_Val[:,ini:ini+dim].min(axis=1) \n",
    "                       for i in range(ini,ini+dim)]) / rng\n",
    "\n",
    "rng = (dfX_Val[:,ecg+ini:ini+ecg+dim].max(axis=1) - dfX_Val[:,ecg+ini:ini+ecg+dim].min(axis=1))\n",
    "valX_1_n = np.array([dfX_Val [:,i] - dfX_Val[:,ini+ecg:ini+ecg+dim].min(axis=1) for i in range(ini+ecg,ini+ecg+dim)]) / rng\n",
    "\n",
    "rng = (dfX_Test[:,ini:ini+dim].max(axis=1) - dfX_Test[:,ini:ini+dim].min(axis=1))\n",
    "testX_0_n = np.array([dfX_Test [:,i] - dfX_Test[:,ini:ini+dim].min(axis=1) \n",
    "                       for i in range(ini,ini+dim)]) / rng\n",
    "\n",
    "rng = (dfX_Test[:,ecg+ini:ini+ecg+dim].max(axis=1) - dfX_Test[:,ecg+ini:ini+ecg+dim].min(axis=1))\n",
    "testX_1_n = np.array([dfX_Test [:,i] - dfX_Test[:,ini+ecg:ini+ecg+dim].min(axis=1) for i in range(ini+ecg,ini+ecg+dim)]) / rng\n",
    "\n",
    "trainX_0_n = trainX_0_n.T\n",
    "trainX_1_n = trainX_1_n.T\n",
    "valX_0_n = valX_0_n.T\n",
    "valX_1_n = valX_1_n.T\n",
    "testX_0_n = testX_0_n.T \n",
    "testX_1_n = testX_1_n.T\n",
    "\n",
    "print(trainX_0_n.shape,trainX_1_n.shape)\n",
    "print(valX_0_n.shape,valX_1_n.shape)\n",
    "print(testX_0_n.shape,testX_1_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8126, 250, 2) (1016, 250, 2) (1016, 250, 2) (8126, 250) (1016, 250) (1016, 250)\n"
     ]
    }
   ],
   "source": [
    "trainX_n = np.stack((trainX_0_n, trainX_1_n), axis=2)\n",
    "valX_n = np.stack((valX_0_n,valX_1_n), axis=2)\n",
    "testX_n = np.stack((testX_0_n, testX_1_n), axis=2)\n",
    "\n",
    "print(trainX_n.shape, valX_n.shape, testX_n.shape, trainY_n.shape, valY_n.shape, testY_n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (64, 250, 150)            91800     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (64, 150)                 180600    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, 250)                 37750     \n",
      "=================================================================\n",
      "Total params: 310,150\n",
      "Trainable params: 310,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#bs=100\n",
    "#look_back = 125*5\n",
    "model = Sequential()\n",
    "model.add(LSTM(150, batch_input_shape=(64,dim,2),dropout=0.4, stateful=True, return_sequences=True))\n",
    "model.add(LSTM(150, dropout=0.4, stateful=True))\n",
    "model.add(Dense(dim))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 960 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 95s 12ms/step - loss: 0.0924 - val_loss: 0.0757\n",
      "Train on 8000 samples, validate on 960 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 93s 12ms/step - loss: 0.0740 - val_loss: 0.0678\n",
      "Train on 8000 samples, validate on 960 samples\n",
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 93s 12ms/step - loss: 0.0678 - val_loss: 0.0574\n",
      "Train on 8000 samples, validate on 960 samples\n",
      "Epoch 1/1\n",
      "1280/8000 [===>..........................] - ETA: 1:15 - loss: 0.0621"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = []\n",
    "model.compile(optimizer=optimizers.Adam(lr=1e-2), loss='mse')\n",
    "for i in range(5):\n",
    "    history.append(model.fit(trainX_n[:8000], trainY_n[:8000], validation_data=(valX_n[:960], valY_n[:960]), \n",
    "                        batch_size= 64, epochs=1, verbose=1))\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEsdJREFUeJzt3X2MXXWdx/H3l7a2lLKlnbZYOnRbA3EtYECuVQKbgNonDBTFZashNruamqwkPgRDu6w86SbAqhCiQKqSEM3ysBBCN2BoizSaVYEpdGPLgzNUTIeyUFtgLVAU9rt/zAHvb7zDTOfemduh71dyc885v+858/0xCZ8559x7GpmJJElvOqTdDUiSDiwGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgrj293AcMyYMSPnzZvX7jYkaUzZvHnz7zNz5mB1YzIY5s2bR1dXV7vbkKQxJSJ+N5Q6LyVJkgoGgySpYDBIkgpj8h6DJO2vP/3pT/T29rJv3752tzLiJk2aRGdnJxMmTBjW/gaDpINCb28vhx9+OPPmzSMi2t3OiMlMdu/eTW9vL/Pnzx/WMbyUJOmgsG/fPjo6Ot7RoQAQEXR0dDR1ZmQwSDpovNND4U3NztNgkCQVDAZJGgUvvvgi119//X7vd+aZZ/Liiy+OQEcDMxgkaRQMFAxvvPHG2+537733csQRR4xUWw35qSRJGgWrV6/mqaee4sQTT2TChAlMmTKF2bNns2XLFh577DHOOeccduzYwb59+/jSl77EqlWrgD8/Amjv3r0sW7aM0047jV/84hfMmTOHu+++m0MPPbTlvRoMkg46l//nNh7b+b8tPeaCo/6KS886bsDxK6+8kq1bt7JlyxY2bdrExz/+cbZu3frWR0pvuukmpk+fzquvvsoHP/hBzj33XDo6OopjdHd3c8stt/D973+f8847jzvvvJPzzz+/pfMAg0GS2mLhwoXF9wyuu+467rrrLgB27NhBd3f3XwTD/PnzOfHEEwE4+eSTefrpp0ekN4NB0kHn7f6yHy2HHXbYW8ubNm1i48aN/PKXv2Ty5MmcfvrpDb+HMHHixLeWx40bx6uvvjoivXnzWZJGweGHH84f/vCHhmMvvfQS06ZNY/LkyTzxxBP86le/GuXuSp4xSNIo6Ojo4NRTT+X444/n0EMP5cgjj3xrbOnSpdx44428//3v573vfS8f/vCH29gpRGa2tYHhqNVq6T/UI2l/PP7447zvfe9rdxujptF8I2JzZtYG29dLSZKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJI0Cob72G2Aa6+9lldeeaXFHQ2sJcEQEUsj4smI6ImI1Q3GJ0bEbdX4gxExr9/43IjYGxEXtqIfSTrQjKVgaPqbzxExDvgesAjoBR6OiHWZ+Vhd2eeAFzLzmIhYAVwF/H3d+DXAT5rtRZIOVPWP3V60aBGzZs3i9ttv57XXXuMTn/gEl19+OS+//DLnnXcevb29vPHGG3z961/nueeeY+fOnZxxxhnMmDGDBx54YMR7bcUjMRYCPZm5HSAibgWWA/XBsBy4rFq+A/huRERmZkScA2wHXm5BL5I0uJ+shv/5dWuP+e4TYNmVAw7XP3Z7/fr13HHHHTz00ENkJmeffTY/+9nP2LVrF0cddRT33HMP0PcMpalTp/Kd73yHBx54gBkzZrS25wG04lLSHGBH3Xpvta1hTWa+DrwEdETEYcBFwOUt6EOSxoT169ezfv16TjrpJD7wgQ/wxBNP0N3dzQknnMDGjRu56KKL+PnPf87UqVPb0l8rzhiiwbb+D2AaqOZy4JrM3BvRqKTuABGrgFUAc+fOHUabklR5m7/sR0NmsmbNGr7whS/8xdjmzZu59957WbNmDYsXL+aSSy4Z9f5accbQCxxdt94J7ByoJiLGA1OBPcCHgKsj4mngy8A/R8QFjX5IZq7NzFpm1mbOnNmCtiVp9NQ/dnvJkiXcdNNN7N27F4BnnnmG559/np07dzJ58mTOP/98LrzwQh555JG/2Hc0tOKM4WHg2IiYDzwDrAA+069mHbAS+CXwKeCn2fdY1799syAiLgP2ZuZ3W9CTJB1Q6h+7vWzZMj7zmc9wyimnADBlyhR+/OMf09PTw9e+9jUOOeQQJkyYwA033ADAqlWrWLZsGbNnzx6Vm88teex2RJwJXAuMA27KzH+NiCuArsxcFxGTgB8BJ9F3prDizZvVdce4jL5g+NZgP8/HbkvaXz52e+iP3W7JP9STmfcC9/bbdknd8j7g7wY5xmWt6EWS1By/+SxJKhgMkg4aY/FfrByOZudpMEg6KEyaNIndu3e/48MhM9m9ezeTJk0a9jFaco9Bkg50nZ2d9Pb2smvXrna3MuImTZpEZ2fnsPc3GCQdFCZMmMD8+fPb3caY4KUkSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFQwGSVLBYJAkFVoSDBGxNCKejIieiFjdYHxiRNxWjT8YEfOq7YsiYnNE/Lp6/0gr+pEkDV/TwRAR44DvAcuABcCnI2JBv7LPAS9k5jHANcBV1fbfA2dl5gnASuBHzfYjSWpOK84YFgI9mbk9M/8I3Aos71ezHLi5Wr4D+GhERGY+mpk7q+3bgEkRMbEFPUmShqkVwTAH2FG33ltta1iTma8DLwEd/WrOBR7NzNda0JMkaZjGt+AY0WBb7k9NRBxH3+WlxQP+kIhVwCqAuXPn7n+XkqQhacUZQy9wdN16J7BzoJqIGA9MBfZU653AXcBnM/OpgX5IZq7NzFpm1mbOnNmCtiVJjbQiGB4Gjo2I+RHxLmAFsK5fzTr6bi4DfAr4aWZmRBwB3AOsycz/akEvkqQmNR0M1T2DC4D7gMeB2zNzW0RcERFnV2U/BDoiogf4KvDmR1ovAI4Bvh4RW6rXrGZ7kiQNX2T2vx1w4KvVatnV1dXuNiRpTImIzZlZG6zObz5LkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySp0JJgiIilEfFkRPRExOoG4xMj4rZq/MGImFc3tqba/mRELGlFP5Kk4Ws6GCJiHPA9YBmwAPh0RCzoV/Y54IXMPAa4Briq2ncBsAI4DlgKXF8dT5LUJq04Y1gI9GTm9sz8I3ArsLxfzXLg5mr5DuCjERHV9lsz87XM/C3QUx1PktQmrQiGOcCOuvXealvDmsx8HXgJ6BjivpKkUdSKYIgG23KINUPZt+8AEasioisiunbt2rWfLUqShqoVwdALHF233gnsHKgmIsYDU4E9Q9wXgMxcm5m1zKzNnDmzBW1LkhppRTA8DBwbEfMj4l303Uxe169mHbCyWv4U8NPMzGr7iupTS/OBY4GHWtCTJGmYxjd7gMx8PSIuAO4DxgE3Zea2iLgC6MrMdcAPgR9FRA99Zworqn23RcTtwGPA68AXM/ONZnuSJA1f9P3hPrbUarXs6upqdxuSNKZExObMrA1W5zefJUkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEmFpoIhIqZHxIaI6K7epw1Qt7Kq6Y6IldW2yRFxT0Q8ERHbIuLKZnqRJLVGs2cMq4H7M/NY4P5qvRAR04FLgQ8BC4FL6wLkW5n5N8BJwKkRsazJfiRJTWo2GJYDN1fLNwPnNKhZAmzIzD2Z+QKwAViama9k5gMAmflH4BGgs8l+JElNajYYjszMZwGq91kNauYAO+rWe6ttb4mII4Cz6DvrkCS10fjBCiJiI/DuBkMXD/FnRINtWXf88cAtwHWZuf1t+lgFrAKYO3fuEH+0JGl/DRoMmfmxgcYi4rmImJ2Zz0bEbOD5BmW9wOl1653Aprr1tUB3Zl47SB9rq1pqtVq+Xa0kafiavZS0DlhZLa8E7m5Qcx+wOCKmVTedF1fbiIhvAlOBLzfZhySpRZoNhiuBRRHRDSyq1omIWkT8ACAz9wDfAB6uXldk5p6I6KTvctQC4JGI2BIRn2+yH0lSkyJz7F2VqdVq2dXV1e42JGlMiYjNmVkbrM5vPkuSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKnQVDBExPSI2BAR3dX7tAHqVlY13RGxssH4uojY2kwvkqTWaPaMYTVwf2YeC9xfrRciYjpwKfAhYCFwaX2ARMQngb1N9iFJapFmg2E5cHO1fDNwToOaJcCGzNyTmS8AG4ClABExBfgq8M0m+5AktUizwXBkZj4LUL3PalAzB9hRt95bbQP4BvBt4JUm+5Aktcj4wQoiYiPw7gZDFw/xZ0SDbRkRJwLHZOZXImLeEPpYBawCmDt37hB/tCRpfw0aDJn5sYHGIuK5iJidmc9GxGzg+QZlvcDpdeudwCbgFODkiHi66mNWRGzKzNNpIDPXAmsBarVaDta3JGl4mr2UtA5481NGK4G7G9TcByyOiGnVTefFwH2ZeUNmHpWZ84DTgN8MFAqSpNHTbDBcCSyKiG5gUbVORNQi4gcAmbmHvnsJD1evK6ptkqQDUGSOvasytVotu7q62t2GJI0pEbE5M2uD1fnNZ0lSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSwWCQJBUMBklSITKz3T3st4jYBfyu3X3spxnA79vdxChzzgcH5zx2/HVmzhysaEwGw1gUEV2ZWWt3H6PJOR8cnPM7j5eSJEkFg0GSVDAYRs/adjfQBs754OCc32G8xyBJKnjGIEkqGAwtFBHTI2JDRHRX79MGqFtZ1XRHxMoG4+siYuvId9y8ZuYcEZMj4p6IeCIitkXElaPb/f6JiKUR8WRE9ETE6gbjEyPitmr8wYiYVze2ptr+ZEQsGc2+mzHcOUfEoojYHBG/rt4/Mtq9D0czv+NqfG5E7I2IC0er5xGRmb5a9AKuBlZXy6uBqxrUTAe2V+/TquVpdeOfBP4d2Nru+Yz0nIHJwBlVzbuAnwPL2j2nAeY5DngKeE/V638DC/rV/BNwY7W8AritWl5Q1U8E5lfHGdfuOY3wnE8CjqqWjweeafd8RnK+deN3Av8BXNju+TTz8oyhtZYDN1fLNwPnNKhZAmzIzD2Z+QKwAVgKEBFTgK8C3xyFXltl2HPOzFcy8wGAzPwj8AjQOQo9D8dCoCczt1e93krf3OvV/7e4A/hoRES1/dbMfC0zfwv0VMc70A17zpn5aGburLZvAyZFxMRR6Xr4mvkdExHn0PdHz7ZR6nfEGAytdWRmPgtQvc9qUDMH2FG33lttA/gG8G3glZFsssWanTMAEXEEcBZw/wj12axB51Bfk5mvAy8BHUPc90DUzJzrnQs8mpmvjVCfrTLs+UbEYcBFwOWj0OeIG9/uBsaaiNgIvLvB0MVDPUSDbRkRJwLHZOZX+l+3bLeRmnPd8ccDtwDXZeb2/e9wVLztHAapGcq+B6Jm5tw3GHEccBWwuIV9jZRm5ns5cE1m7q1OIMY0g2E/ZebHBhqLiOciYnZmPhsRs4HnG5T1AqfXrXcCm4BTgJMj4mn6fi+zImJTZp5Om43gnN+0FujOzGtb0O5I6QWOrlvvBHYOUNNbhd1UYM8Q9z0QNTNnIqITuAv4bGY+NfLtNq2Z+X4I+FREXA0cAfxfROzLzO+OfNsjoN03Od5JL+DfKG/EXt2gZjrwW/puvk6rlqf3q5nH2Ln53NSc6bufcidwSLvnMsg8x9N3/Xg+f74xeVy/mi9S3pi8vVo+jvLm83bGxs3nZuZ8RFV/brvnMRrz7VdzGWP85nPbG3gnvei7tno/0F29v/k/vxrwg7q6f6TvBmQP8A8NjjOWgmHYc6bvL7IEHge2VK/Pt3tObzPXM4Hf0PfJlYurbVcAZ1fLk+j7REoP8BDwnrp9L672e5ID9JNXrZwz8C/Ay3W/1y3ArHbPZyR/x3XHGPPB4DefJUkFP5UkSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkgsEgSSoYDJKkwv8DSd6momK3uxcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for i in range(2):\n",
    "    if i == 0:\n",
    "        if history_prev is None:\n",
    "            continue\n",
    "        else:\n",
    "            hist = history_prev\n",
    "    else:\n",
    "        hist = history\n",
    "        \n",
    "    loss = hist.history[\"loss\"]\n",
    "    val_loss = hist.history[\"val_loss\"]\n",
    "    epochs = range(1, len(loss) +1)\n",
    "    ax = plt.subplot(1,2,1+i)\n",
    "    ax.plot(epochs, loss, label=\"train\")\n",
    "    ax.plot(epochs, val_loss, label=\"validation\")\n",
    "    ax.set_xticks(epochs, epochs)\n",
    "    ax.set_ylim(0.0,1.0)\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('conv1d.h5')\n",
    "with open('conv1d.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for ix in range(testX_n.shape[0]):\n",
    "    y_pred.append(model.predict([np.expand_dims(testX_n[ix,:].reshape(250,2),0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_pred = np.array(y_pred).reshape(-1,250)\n",
    "print(y_pred.shape, testY_n.shape)\n",
    "print(\"Test error analysis:\")\n",
    "print(\"Explained variance score:\", explained_variance_score(testY_n, y_pred))\n",
    "print(\"MAE error:\", mean_absolute_error(testY_n, y_pred))\n",
    "print(\"MSE error:\", mean_squared_error(testY_n, y_pred))\n",
    "print(\"R2 score:\", r2_score(testY_n, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix=10\n",
    "print(np.expand_dims(testX_n[ix,:], axis=0).shape)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "ax = plt.subplot(3,1,1)\n",
    "ax.plot(testX_0_n[ix,:].reshape(1,-1).T, c=\"g\", label=\"PPG\")\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(3,1,2)\n",
    "ax.plot(testX_1_n[ix,:].reshape(1,-1).T, c=\"g\", label=\"ECG\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "ax = plt.subplot(3,1,3)\n",
    "ax.plot(model.predict(np.expand_dims(testX_n[ix,:], axis=0)).T, label=\"pred_ART\")\n",
    "ax.plot(testY_n[ix,:].reshape(1,-1).T, c=\"g\", label=\"real_ART\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
